{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d46fb169",
   "metadata": {},
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29fa2c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import pandas as pd\n",
    "\n",
    "from utils.prompts import render\n",
    "from utils.llm_client import LLMClient\n",
    "from utils.logging_utils import log_llm_call\n",
    "from utils.router import pick_model,should_use_reasoning_model\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937a829e",
   "metadata": {},
   "source": [
    "### Loading data from sample.txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "253bd759",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../data/raw/Incidents.txt'\n",
    "\n",
    "incidents = []\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Skip header\n",
    "header = [h.strip() for h in lines[0].split('|')]\n",
    "\n",
    "for line in lines[1:]:\n",
    "    if line.strip() == \"\":\n",
    "        continue  # skip empty lines\n",
    "    values = [v.strip() for v in line.split('|')]\n",
    "    entry = dict(zip(header, values))\n",
    "    \n",
    "    # Convert numeric fields\n",
    "    entry['ID'] = int(entry['ID'])\n",
    "    entry['People'] = int(entry['People'])\n",
    "    \n",
    "    # Convert Ages to list of integers\n",
    "    entry['Ages'] = [str(a.strip()) for a in entry['Ages'].split(',')]\n",
    "    \n",
    "    incidents.append(entry)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d6ad78",
   "metadata": {},
   "source": [
    "### Assign a Priority Score for each incidernt by COT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc63e532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are emergency_triage_specialist. Apply the specified crisis-prioritization rules exactly.\n",
      "Problem: {incident}\n",
      "\n",
      "First, outline your reasoning steps briefly.\n",
      "Then provide the final answer clearly marked under 'Answer:'.\n",
      "Keep reasoning concise; avoid unnecessary prose.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = pick_model('openai', 'cot')\n",
    "client = LLMClient('openai', model)\n",
    "\n",
    "instruction = \"\"\"\n",
    "Calculate the Priority Score (1-10) for the incident based on these rules:\n",
    "1. Base Score: 5.\n",
    "2. Age Factor: +2 if age > 60 or < 5.\n",
    "3. Life Threat: +3 if Need is \"Rescue\".\n",
    "4. Medical: +1 if Need is \"Insulin\" or \"Medicine\".\n",
    "\n",
    "STRICT OUTPUT RULE:\n",
    "- Provide ONLY the score in the exact format shown below.\n",
    "- NO reasoning, NO Incident IDs, NO labels, NO extra text.\n",
    "\n",
    "Format:\n",
    "Score: [Score]/10\n",
    "\"\"\"\n",
    "\n",
    "prompt_text, spec = render(\n",
    "    'cot_reasoning.v1',\n",
    "    role='emergency_triage_specialist',\n",
    "    problem=\"{incident}\"\n",
    ")\n",
    "\n",
    "print(prompt_text) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e1fc83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Incident 1 ===\n",
      "\n",
      "=== Incident 2 ===\n",
      "\n",
      "=== Incident 3 ===\n",
      "{'ID': 1, 'Time': '08:00 AM', 'Area': 'Gampaha', 'People': 4, 'Ages': ['20-40'], 'Main Need': 'Water', 'Message': '\"Thirsty but safe on roof. Water level stable.\"', 'Score': 'Score: 5/10'}\n",
      "{'ID': 2, 'Time': '08:15 AM', 'Area': 'Ja-Ela', 'People': 1, 'Ages': ['75'], 'Main Need': 'Insulin', 'Message': '\"Diabetic, missed dose yesterday. Feeling faint.\"', 'Score': 'Score: 8/10'}\n",
      "{'ID': 3, 'Time': '08:20 AM', 'Area': 'Ragama', 'People': 2, 'Ages': ['10', '35'], 'Main Need': 'Rescue', 'Message': '\"Water approaching neck level. Child is crying.\"', 'Score': 'Score: 8/10'}\n"
     ]
    }
   ],
   "source": [
    "incident_scores = []\n",
    "for i, incident in enumerate(incidents, 1):\n",
    "    full_prompt = f\"\"\"text: {prompt_text.replace(\"{incident}\", str(incident))}\n",
    "    instruction: {instruction}\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": full_prompt}\n",
    "    ]\n",
    "\n",
    "    print(f\"\\n=== Incident {i} ===\")\n",
    "    response = client.chat(messages, temperature=0, max_tokens=spec.max_tokens)\n",
    "    incident_scores.append(response['text'])\n",
    "    log_llm_call('openai', model, 'cot', response['latency_ms'], response['usage'])\n",
    "\n",
    "# updating existing data with scores\n",
    "for i, incident in enumerate(incidents, 1):\n",
    "    incident['Score'] = incident_scores[i-1]\n",
    "    print(incident)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99d24e3",
   "metadata": {},
   "source": [
    "### ToT - Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef53558d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToT Response (Multiple Solution Paths):\n",
      "================================================================================\n",
      "Below is one way to “think like a Logistics Commander” by developing three alternative solution branches. In our scenario, we must rescue three incidents from Ragama with one boat. We know that:\n",
      "\n",
      "• Incident 1 – Gampaha, 4 people, main need “Water,” Score 5/10 (time 08:00 AM)  \n",
      "• Incident 2 – Ja-Ela, 1 person (elderly needs Insulin), Score 8/10 (time 08:15 AM)  \n",
      "• Incident 3 – Ragama, 2 people (child and adult; rescue needed as water reaches neck level), Score 8/10 (time 08:20 AM)\n",
      "\n",
      "Travel times provided:  \n",
      "– Ragama to Ja-Ela = 10 minutes  \n",
      "– Ja-Ela to Gampaha = 40 minutes  \n",
      "(Thus, Ragama to Gampaha “directly” takes an effective 10+40 = 50 minutes.)\n",
      "\n",
      "The overall goal is to “maximize the total priority score saved (i.e. 21 points in sum if all are handled) within the shortest time.”\n",
      "\n",
      "Let’s explore each branch with its own hypothesis, steps, and an intermediate check.\n",
      "\n",
      "──────────────────────────────\n",
      "Branch 1 (Greedy: Save Highest Score First)\n",
      "\n",
      "Hypothesis:\n",
      "• Give priority to incidents with the highest score first. Here, two incidents have Score = 8/10 (Incident 2 and Incident 3) versus Incident 1’s 5/10.\n",
      "• When scores tie, the one that costs less travel time (or is already at our starting base) should be looked at first since it yields faster rescue without sacrificing score.\n",
      "\n",
      "Steps:\n",
      "1. Although Incident 2 and Incident 3 are tied on score (8/10), note that Incident 3 is in Ragama where you already are. So rescue Incident 3 first. (Also note its message “water approaching neck level …” which adds to its urgency.)\n",
      "2. Next, head from Ragama to Ja-Ela (10 min travel) and handle Incident 2.\n",
      "3. Finally, travel from Ja-Ela to Gampaha (40 min travel) and attend Incident 1.\n",
      "\n",
      "Intermediate Check:\n",
      "• Total travel time = 0 (Ragama already) + 10 + 40 = 50 minutes.  \n",
      "• Rescued scores: 8 + 8 + 5 = 21 points.\n",
      "• This route gives high-priority rescues first while keeping travel times minimal.\n",
      "──────────────────────────────\n",
      "Branch 2 (Speed: Save Closest First)\n",
      "\n",
      "Hypothesis:\n",
      "• Minimize time by addressing incidents based solely on proximity from your starting point.\n",
      "• From Ragama, pick the incident that requires no travel, then the closest among the remaining.\n",
      "\n",
      "Steps:\n",
      "1. Rescue Incident 3 in Ragama immediately (0-min travel).\n",
      "2. The next closest is Ja-Ela (10 minutes from Ragama) for Incident 2.\n",
      "3. The furthest by distance is Gampaha. After Ja-Ela, travel the 40 minutes to handle Incident 1.\n",
      "\n",
      "Intermediate Check:\n",
      "• Total travel time remains: 0 + 10 + 40 = 50 minutes.\n",
      "• The order (Incident 3 → Incident 2 → Incident 1) is identical to Branch 1 in outcome, and you secure all 21 points in the same 50 minutes.\n",
      "──────────────────────────────\n",
      "Branch 3 (Logistics: Save Furthest First)\n",
      "\n",
      "Hypothesis:\n",
      "• In this branch, “furthest” as measured by travel distance from Ragama is given priority. The thinking might be “if the far end is delayed, get it done first.”\n",
      "• The furthest location is Gampaha.\n",
      "\n",
      "Steps:\n",
      "1. First travel directly from Ragama to Gampaha (50 minutes) to handle Incident 1.\n",
      "2. Then, go from Gampaha to Ja-Ela, which takes 40 minutes, to attend Incident 2.\n",
      "3. Finally, return from Ja-Ela to Ragama (10 minutes) to deal with Incident 3.\n",
      "\n",
      "Intermediate Check:\n",
      "• Total travel time = 50 + 40 + 10 = 100 minutes.\n",
      "• Although you rescue all incidents (total score 21), the long route doubles the travel time compared to Branches 1 and 2.\n",
      "──────────────────────────────\n",
      "Comparison and Final Decision\n",
      "\n",
      "• Branches 1 and 2 both yield the same route and total travel time (50 minutes) while securing the highest overall score of 21.  \n",
      "• Branch 3 (furthest first) is much less time efficient (100 minutes) even though it still covers all incidents.\n",
      "\n",
      "Thus, the optimal route according to both a “greedy” priority (high scores) and “speed” (short travel distances) approaches is the same:  \n",
      "1. Rescue Incident 3 immediately in Ragama  \n",
      "2. Travel 10 minutes to Ja-Ela for Incident 2  \n",
      "3. Finally, travel 40 minutes to Gampaha for Incident 1\n",
      "\n",
      "──────────────────────────────\n",
      "Answer:\n",
      "Optimal Route → Incident 3 (Ragama) → Incident 2 (Ja-Ela) → Incident 1 (Gampaha); Total travel time = 50 minutes with a combined score of 21 points.\n",
      "\n",
      "This is the route that maximizes priority (by quickly handling two 8/10 alerts) while securing all rescues in the shortest possible time.\n"
     ]
    }
   ],
   "source": [
    "model_tot = pick_model('openai', 'tot')\n",
    "client_tot = LLMClient('openai', model_tot)\n",
    "\n",
    "incidents_data_string = \"\\n\".join([str(inc) for inc in incidents])\n",
    "\n",
    "problem_statement = f\"\"\"\n",
    "SITUATION: \n",
    "You are at Ragama with ONE rescue boat. \n",
    "There are serveral incidents to handle:\n",
    "{incidents_data_string}\n",
    "\n",
    "TRAVEL TIMES:\n",
    "- Ragama to Ja-Ela: 10 mins\n",
    "- Ja-Ela to Gampaha: 40 mins\n",
    "(Note: Ragama to Gampaha would be 10+40 = 50 mins)\n",
    "\n",
    "GOAL:\n",
    "Maximize the total priority score saved within the shortest time. \n",
    "The boat has unlimited capacity but must stop at each location to complete the incident.\n",
    "\n",
    "TASK:\n",
    "Explore 3 distinct branches:\n",
    "1. Branch 1 (Greedy): Save highest score first.\n",
    "2. Branch 2 (Speed): Save closest first.\n",
    "3. Branch 3 (Logistics): Save furthest first.\n",
    "\n",
    "Compare all three and select the optimal route.\n",
    "\"\"\"\n",
    "\n",
    "prompt_text, spec = render(\n",
    "    'tot_reasoning.v1',\n",
    "    role='Logistics Commander',\n",
    "    problem=problem_statement,\n",
    "    branches='3'\n",
    ")\n",
    "\n",
    "messages = [{'role': 'user', 'content': prompt_text}]\n",
    "response = client_tot.chat(messages, temperature=spec.temperature, max_tokens=spec.max_tokens)\n",
    "\n",
    "print('ToT Response (Multiple Solution Paths):')\n",
    "print('=' * 80)\n",
    "print(response['text'])\n",
    "log_llm_call('openai', model_tot, 'tot', response['latency_ms'], response['usage'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57905a27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f423d5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
